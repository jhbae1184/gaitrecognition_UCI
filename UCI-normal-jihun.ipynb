{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe327860886d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('../')\n",
    "import Processing, Model\n",
    "\n",
    "\n",
    "def call_data(base_path, sub_lst, sub_idx):\n",
    "    data = pd.read_csv(base_path+sub_lst[sub_idx])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d260b6a4ab53c91",
   "metadata": {},
   "source": [
    "# Normal 용도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_setup(lst_normal, path_normal, window_size, step_size):\n",
    "    subjects_data = defaultdict(list)\n",
    "    subjects_label = defaultdict(list)\n",
    "\n",
    "    for s_idx, filename in enumerate(lst_normal):\n",
    "        name = filename.lower()\n",
    "\n",
    "        subject_id = name.split(\"n\")[0]\n",
    "        data = call_data(path_normal, lst_normal, s_idx).to_numpy()  # (num_samples, num_channels)\n",
    "\n",
    "        windows = Processing.sliding_window(data, window_size, step_size)  # (num_windows, win_len, ch)\n",
    "\n",
    "        if \"standing\" in name: label = 0\n",
    "        elif \"gait\" in name: label = 1\n",
    "        elif \"sitting\" in name: label = 2\n",
    "        else: label = -1\n",
    "        #print(name, label)\n",
    "        labels = np.full(len(windows), label)\n",
    "\n",
    "        subjects_data[subject_id].append(windows)\n",
    "        subjects_label[subject_id].append(labels)\n",
    "\n",
    "    return subjects_data, subjects_label\n",
    "\n",
    "\n",
    "def get_X_y(X, y):\n",
    "    all_X, all_y = [], []\n",
    "\n",
    "    #for subject_id in subjects_data.keys():\n",
    "    data_list = X\n",
    "    label_list = y\n",
    "\n",
    "    for data, labels in zip(data_list, label_list):\n",
    "        for w, label in zip(data, labels):  # w: (win_len, ch)\n",
    "            feat = Processing.extract_features_WL(w)  # (num_channels*5,)\n",
    "            all_X.append(feat)\n",
    "            all_y.append(label)\n",
    "\n",
    "    all_X, all_y = np.array(all_X), np.array(all_y)   # (N, num_channels*5)\n",
    "\n",
    "    return all_X, all_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de9d90a66b08b4",
   "metadata": {},
   "source": [
    "# Abnormal 용도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12846884a8e993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_setup(lst_normal, path_normal, window_size, step_size):\n",
    "    subjects_data = defaultdict(list)\n",
    "    subjects_label = defaultdict(list)\n",
    "\n",
    "    for s_idx, filename in enumerate(lst_normal):\n",
    "        name = filename.lower()\n",
    "\n",
    "        subject_id = name.split(\"a\")[0]\n",
    "        data = call_data(path_normal, lst_normal, s_idx).to_numpy()  # (num_samples, num_channels)\n",
    "\n",
    "        windows = Processing.sliding_window(data, window_size, step_size)  # (num_windows, win_len, ch)\n",
    "\n",
    "        if \"standing\" in name: label = 0\n",
    "        elif \"gait\" in name: label = 1\n",
    "        elif \"sitting\" in name: label = 2\n",
    "        else: label = -1\n",
    "        #print(name, label)\n",
    "        labels = np.full(len(windows), label)\n",
    "\n",
    "        subjects_data[subject_id].append(windows)\n",
    "        subjects_label[subject_id].append(labels)\n",
    "\n",
    "    return subjects_data, subjects_label\n",
    "\n",
    "\n",
    "def get_X_y(X, y):\n",
    "    all_X, all_y = [], []\n",
    "\n",
    "    #for subject_id in subjects_data.keys():\n",
    "    data_list = X\n",
    "    label_list = y\n",
    "\n",
    "    for data, labels in zip(data_list, label_list):\n",
    "        for w, label in zip(data, labels):  # w: (win_len, ch)\n",
    "            #feat = processing.extract_features(w)  # (num_channels*5,)\n",
    "            feat = Processing.extract_features_WL(w)\n",
    "            if len(feat) > 5: ##########################################MAKE SURE to attach this when running ABnormal\n",
    "                feat = feat[:5]\n",
    "            all_X.append(feat)\n",
    "            all_y.append(label)\n",
    "\n",
    "    all_X, all_y = np.array(all_X), np.array(all_y)   # (N, num_channels*5)\n",
    "    #print(pd.Series(all_y).value_counts())\n",
    "    return all_X, all_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9d77cc15505285",
   "metadata": {},
   "source": [
    "# Inter-subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc3b7f128ffa9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "PATH_ABNORMAL = r\"C:/Users/hml76/PycharmProjects/Cross-motor-decoding/Data/Gait1-UCI/Abnormal/\"\n",
    "PATH_NORMAL   = r\"C:/Users/hml76/PycharmProjects/Cross-motor-decoding/Data/Gait1-UCI/normal/\"\n",
    "\n",
    "WINDOW_SIZE = 200\n",
    "STEP_SIZE   = 10\n",
    "NUM_CHANNELS = 5\n",
    "NUM_FEATURES = 30\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "SUBJECT_LIST = [str(i) for i in range(1, 12)]   # '1' ~ '11'\n",
    "\n",
    "normal_files = os.listdir(PATH_NORMAL)\n",
    "subjects_data, subjects_label = data_setup(\n",
    "    normal_files, PATH_NORMAL, WINDOW_SIZE, STEP_SIZE\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# LOSO Training Loop - 아까말한 leave one subject out\n",
    "# -----------------------\n",
    "accuracy_list = []\n",
    "\n",
    "for test_id in SUBJECT_LIST:\n",
    "    print(f\"\\n===== Test Subject: {test_id} =====\")\n",
    "\n",
    "    # Test set\n",
    "    X_test, y_test = get_X_y(subjects_data[test_id], subjects_label[test_id])\n",
    "    X_test = tf.expand_dims(X_test, axis=-1)\n",
    "\n",
    "    # Train set: concatenate all other subjects\n",
    "    train_X, train_y = [], []\n",
    "    for sub_id in SUBJECT_LIST:\n",
    "        if sub_id == test_id:\n",
    "            continue\n",
    "        X_tmp, y_tmp = get_X_y(subjects_data[sub_id], subjects_label[sub_id])\n",
    "        train_X.append(X_tmp)\n",
    "        train_y.append(y_tmp)\n",
    "\n",
    "    X_train = np.concatenate(train_X, axis=0)\n",
    "    y_train = np.concatenate(train_y, axis=0)\n",
    "    X_train = tf.expand_dims(X_train, axis=-1)\n",
    "\n",
    "    print(\"Train:\", X_train.shape, y_train.shape)\n",
    "    print(\"Test: \", X_test.shape, y_test.shape)\n",
    "\n",
    "    model = Model.build_model_1D(NUM_CLASSES, X_train.shape[1:])\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        batch_size=256,\n",
    "        epochs=100,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['accuracy'], label=\"Train Acc\")\n",
    "    plt.plot(history.history['val_accuracy'], label=\"Val Acc\")\n",
    "    plt.title(f\"Subject {test_id} Learning Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy = {test_acc * 100:.2f}%\")\n",
    "    accuracy_list.append(test_acc)\n",
    "\n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "# Final Results\n",
    "print(\"\\n========== Summary ==========\")\n",
    "for sid, acc in zip(SUBJECT_LIST, accuracy_list):\n",
    "    print(f\"Subject {sid} → {acc*100:.2f}%\")\n",
    "\n",
    "print(f\"Average ACC: {np.mean(accuracy_list)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf0d5b13f7face",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
